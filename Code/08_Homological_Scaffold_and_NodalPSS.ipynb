{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of Homological Scaffold and calculate NodalPSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we calculate Nodal PSS <br>\n",
    "Code courtesy of Homological Scaffold construction: __[G.Petri](https://github.com/lordgrilo/Holes)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re, time\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from random import random\n",
    "import networkx as nx\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.stats.multitest as multitest\n",
    "\n",
    "sys.path.append('../')\n",
    "import Holes as ho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HomoScaffold(G, gen, outfilename):\n",
    "    ScafH = nx.Graph()\n",
    "    # ScafH.add_nodes_from(G)\n",
    "    edges = []\n",
    "    for c in gen[1]:\n",
    "        edges.extend(c.cycles())\n",
    "\n",
    "    for e in edges:\n",
    "        u , v = int(e[0]) , int(e[1])\n",
    "        if ScafH.has_edge(u,v):\n",
    "            ScafH[u][v]['weight'] += 1\n",
    "        else:\n",
    "            ScafH.add_edge(u, v, weight=1)\n",
    "            ScafH[u][v]['persistence'] = 0\n",
    "\n",
    "    for cy in gen[1]:\n",
    "        cyc =  cy.cycles()\n",
    "        unique_integers = [[int(x) for x in sublist] for sublist in cyc]\n",
    "        for sublist in cyc:\n",
    "            u,v = int(sublist[0]) , int(sublist[1])\n",
    "            if ScafH.has_edge(u,v):\n",
    "                ScafH[u][v]['persistence'] += cy.persistence_interval()\n",
    "\n",
    "    with open(outfilename,'wb') as f:\n",
    "        pk.dump(ScafH, f)\n",
    "    return \"Stored Homological Scaffold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "7 [29, 35, 26, 22, 12, 30, 46] ['Visual', 'Somato Motor', 'Dorsal Attention', 'Salient Ventral Attention', 'Limbic', 'Control', 'Default']\n"
     ]
    }
   ],
   "source": [
    "dataset = 'MPI_LEMON'\n",
    "# dataset = 'ABIDE'\n",
    "\n",
    "path_file = f'../Data/{dataset}/FCM_DistMat/'\n",
    "files_list = os.listdir(path_file)\n",
    "print(len(files_list)) \n",
    "\n",
    "RSNs_details = pd.read_csv('../Data/SchaeferAtlas_Regions_details.csv')\n",
    "RSNs7 = RSNs_details['RSN'].unique().tolist()\n",
    "result_dict = {key: RSNs_details.loc[RSNs_details['RSN'] == key, 'Node_number'].tolist() for key in RSNs_details['RSN'].unique()}\n",
    "RSN_node_details = { rsn : {i:result_dict[rsn][i] for i in range(len(result_dict[rsn]))} for rsn in RSNs7}\n",
    "print(len(result_dict),[ len(x) for x in result_dict.values()],RSNs7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holes\n",
    "We first compute the scaffold. <br>\n",
    "Aggregate the homological scaffold and write it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual 29\n",
      "(29, 29) num_components= 1 \t Graph with 29 nodes and 380 edges\n",
      "Preliminary scan of edge weights to define filtration steps...\n",
      "Constructing filtration...\n",
      "Max filtration value: 378\n",
      "Clique dictionary created.\n",
      "Filtration done  1.2714788913726807\n",
      "0\n",
      "persistent_homology_calculation done  8.5032377243042\n",
      "Done for 32301 8.508232116699219 ------------------------------------------------------------\n",
      "Done for  MPI_LEMON 8.508232116699219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Child returned 0\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "path = f'../OutputFiles/PosCorr/{dataset}/Output_RSNs/HomoScaffold/'\n",
    "for RSN in RSNs7:\n",
    "    t1 = time.time()\n",
    "    indices = result_dict[RSN]\n",
    "    print(RSN, len(indices))\n",
    "    rsn = RSN.replace(' ','')\n",
    "    for i in range(len(files_list)):\n",
    "        t2 = time.time()\n",
    "        SubID = files_list[i].split('.')[0].split('_')[-1]\n",
    "        DisMat = pd.read_csv(path_file + files_list[i], header = None, sep = ',').values\n",
    "        DisMat_rsn = DisMat[np.ix_(indices, indices)]\n",
    "        DisMat_rsn[DisMat_rsn > np.sqrt(2)] = 0\n",
    "        G = nx.from_numpy_array(DisMat_rsn)\n",
    "        num_components = nx.number_connected_components(G)\n",
    "        print(DisMat_rsn.shape,'num_components=', num_components,'\\t', G)\n",
    "        fil = ho.filtrations.upward_limited_weight_clique_rank_filtration(G,3)\n",
    "        outpath = path + f'{rsn}/'\n",
    "        os.makedirs(outpath + 'Filtration/', exist_ok= True)\n",
    "        clique_dictionary_file = outpath + f'Filtration/Sub_{SubID}_filtration.pck'\n",
    "        with open(clique_dictionary_file,'wb') as f:\n",
    "            pk.dump(fil,f, protocol=2)\n",
    "        print('Filtration done ', time.time() - t2)\n",
    "        hom_dim = 1\n",
    "        dataset_tag = f'Sub_{SubID}'\n",
    "        output_dir = outpath\n",
    "        \n",
    "        # Compute the generators\n",
    "        ho.persistent_homology_calculation(clique_dictionary_file, hom_dim, dataset_tag, output_dir, m1=512, m2=2048, save_generators=True)\n",
    "        print('persistent_homology_calculation done ', time.time() - t2)\n",
    "        # Dump the results in this file\n",
    "        gen_file = outpath + f'gen/generators_Sub_{SubID}_.pck'\n",
    "        with open(gen_file, 'rb') as f:\n",
    "            gen = pk.load(f)\n",
    "        os.makedirs(outpath + 'Scaffolds/', exist_ok= True)\n",
    "        OutHomScaPath = outpath + f'Scaffolds/JPScaffold_Sub_{SubID}.pck'\n",
    "        homscaf = HomoScaffold(G, gen, OutHomScaPath)\n",
    "        print(f'Done for {SubID}', time.time() - t2, '-'*60 )\n",
    "        break\n",
    "    # print(f'Done for {rsn}', time.time() - t1 )\n",
    "    break\n",
    "print('Done for ', dataset, time.time() - t0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Nodal PSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual 29\n",
      "Somato Motor 35\n",
      "Dorsal Attention 26\n",
      "Salient Ventral Attention 22\n",
      "Limbic 12\n",
      "Control 30\n",
      "Default 46\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "outpath = f'../OutputFiles/PosCorr/{dataset}/Output_RSNs/HomoScaffold/'\n",
    "\n",
    "for RSN in RSNs7:\n",
    "    t1 = time.time()\n",
    "    indices = result_dict[RSN]\n",
    "    NNodes = len(indices)\n",
    "    print(RSN, NNodes)\n",
    "    rsn = RSN.replace(' ','')\n",
    "    NPSSoutdict = {'Nodes': [n for n in range(NNodes)]}\n",
    "    for i in range(len(files_list)):\n",
    "        t2 = time.time()\n",
    "        SubID = files_list[i].split('.')[0].split('_')[-1]\n",
    "        inpath = f'../OutputFiles/PosCorr/{dataset}/Output_RSNs/HomoScaffold/{rsn}/'\n",
    "        OutHomScaPath = inpath + f'Scaffolds/JPScaffold_Sub_{SubID}.pck'\n",
    "        with open(OutHomScaPath, 'rb') as f:\n",
    "            ScafH = pk.load(f)\n",
    "        HSnode = dict(ScafH.degree(weight='persistence'))\n",
    "        NodalPSS = {n: 0 for n in range(NNodes)}\n",
    "        for n in range(NNodes):\n",
    "            if n in HSnode.keys():\n",
    "                NodalPSS[n] = HSnode[n]/2\n",
    "        NPSSoutdict[SubID] = NodalPSS.values()\n",
    "\n",
    "    NPSSfilename = f'{rsn}_NodalPSS_HomoScaffold.csv'\n",
    "    NPSStOutdf = pd.DataFrame(NPSSoutdict)\n",
    "    NPSStOutdf.to_csv(outpath + NPSSfilename, index = None)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------\n",
    "# Function to find mean an standard error \n",
    "#------------------------------------------------\n",
    "\n",
    "def Node_Groupdiff(group1,group2,Infile,g1name,g2name,NNodes):\n",
    "    p_val = []\n",
    "    Gr1,Gr2 = [[],[]],[[],[]]\n",
    "    for i in range(NNodes):\n",
    "        data1,data2 = [],[]\n",
    "        for sub1 in group1:\n",
    "            data1.append(Infile[sub1][i])\n",
    "        Gr1[0].append(np.mean(data1))\n",
    "        Gr1[1].append(stats.sem(data1))\n",
    "        \n",
    "        for sub2 in group2:\n",
    "            data2.append(Infile[sub2][i])\n",
    "        Gr2[0].append(np.mean(data2))\n",
    "        Gr2[1].append(stats.sem(data2))\n",
    "        p_val.append(stats.ttest_ind(data1,data2,equal_var=False)[1])\n",
    "        \n",
    "    correction = multitest.multipletests(p_val, alpha=0.05, method = 'fdr_bh')\n",
    "    fdr = correction[0]\n",
    "    p_v_corrected = correction[1]\n",
    "\n",
    "    nodes = [i for i in range(NNodes)]\n",
    "    Gdiff = {'Nodes': nodes, 'p_values':p_val, 'fdr_corrected_p_val':p_v_corrected, f'{g1name}_mean': Gr1[0], f'{g1name}_sd':Gr1[1], f'{g2name}_mean':Gr2[0], f'{g2name}_sd':Gr2[1]}\n",
    "    return pd.DataFrame(Gdiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young 153 \t Elderly 72 \tTotal: 225\n",
      "32302 32301 ../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/\n"
     ]
    }
   ],
   "source": [
    "if dataset == 'MPI_LEMON':\n",
    "    Detailsfile = pd.read_csv('../Data/MPI_LEMON/MPILemon_Subject_details.csv')\n",
    "    Young = list(map(str, list(Detailsfile.loc[Detailsfile['Cohort'] == 'Young','Subject'])))\n",
    "    Elder = list(map(str, list(Detailsfile.loc[Detailsfile['Cohort'] == 'Elderly','Subject'])))\n",
    "    All_subs = Young + Elder\n",
    "    Group1, Group2 = 'Young', 'Elderly'\n",
    "    GR1, GR2 = Young, Elder\n",
    "\n",
    "elif dataset == 'ABIDE':\n",
    "    Detailsfile = pd.read_csv('../Data/ABIDE/ABIDE_Subject_details.csv')\n",
    "    ASD = list(map(str, list(Detailsfile.loc[Detailsfile['Cohort'] == 'ASD','Subject identifier'])))\n",
    "    Healthy = list(map(str, list(Detailsfile.loc[Detailsfile['Cohort'] == 'HC','Subject identifier'])))\n",
    "    All_subs = ASD + Healthy\n",
    "    Group1, Group2 = 'ASD', 'Healthy'\n",
    "    GR1, GR2 = ASD, Healthy\n",
    "\n",
    "print(Group1, len(GR1), '\\t', Group2, len(GR2), '\\tTotal:', len(GR1+GR2))\n",
    "print(GR1[0], GR2[0], outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 (29, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_Visual_NodalPSS_HomoScaffold.txt \n",
      " Done for Visual  (29, 7)\t Time: 0.09370732307434082\n",
      "35 (35, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_SomatoMotor_NodalPSS_HomoScaffold.txt \n",
      " Done for SomatoMotor  (35, 7)\t Time: 0.18959379196166992\n",
      "26 (26, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_DorsalAttention_NodalPSS_HomoScaffold.txt \n",
      " Done for DorsalAttention  (26, 7)\t Time: 0.2813551425933838\n",
      "22 (22, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_SalientVentralAttention_NodalPSS_HomoScaffold.txt \n",
      " Done for SalientVentralAttention  (22, 7)\t Time: 0.33707427978515625\n",
      "12 (12, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_Limbic_NodalPSS_HomoScaffold.txt \n",
      " Done for Limbic  (12, 7)\t Time: 0.37084388732910156\n",
      "30 (30, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_Control_NodalPSS_HomoScaffold.txt \n",
      " Done for Control  (30, 7)\t Time: 0.454742431640625\n",
      "46 (46, 226)\n",
      "../OutputFiles/PosCorr/MPI_LEMON/Output_RSNs/HomoScaffold/p_values_Default_NodalPSS_HomoScaffold.txt \n",
      " Done for Default  (46, 7)\t Time: 0.5472671985626221\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "for RSN in RSNs7:\n",
    "    t1 = time.time()\n",
    "    indices = result_dict[RSN]\n",
    "    NNodes = len(indices)\n",
    "    rsn = RSN.replace(' ','')\n",
    "    NPSSfilename = f'{rsn}_NodalPSS_HomoScaffold.csv'\n",
    "    Infile = pd.read_csv(outpath + NPSSfilename)\n",
    "    print(NNodes, Infile.shape)\n",
    "    \n",
    "    df_NPSS = Node_Groupdiff(GR1, GR2, Infile, Group1, Group2,NNodes)\n",
    "    df_NPSS = pd.DataFrame(df_NPSS)\n",
    "    df_NPSS.to_csv(outpath + f'p_values_{rsn}_NodalPSS_HomoScaffold.txt',sep = '\\t', index=None)\n",
    "    \n",
    "    print(outpath +f'p_values_{rsn}_NodalPSS_HomoScaffold.txt', f'\\n Done for {rsn}  {df_NPSS.shape}\\t Time:', time.time() - t0)\n",
    "    # break\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI_LEMON 42 [19, 23, 28, 32, 34, 35, 36, 37, 38, 39, 45, 50, 60, 61, 70, 81, 90, 92, 124, 125, 126, 128, 130, 131, 132, 133, 139, 140, 142, 143, 144, 150, 157, 164, 165, 166, 173, 175, 181, 183, 194, 195]\n",
      "Young 153 \t Elderly 72 \tTotal: 225 \n",
      "\n",
      "FDR-corrected: 2 \t Coincide with NIBS: 0 \t Visual\n",
      "FDR-corrected: 26 \t Coincide with NIBS: 11 \t SomatoMotor\n",
      "FDR-corrected: 5 \t Coincide with NIBS: 1 \t DorsalAttention\n",
      "FDR-corrected: 7 \t Coincide with NIBS: 2 \t SalientVentralAttention\n",
      "FDR-corrected: 0 \t Coincide with NIBS: 0 \t Limbic\n",
      "FDR-corrected: 2 \t Coincide with NIBS: 1 \t Control\n",
      "FDR-corrected: 16 \t Coincide with NIBS: 3 \t Default\n",
      "\n",
      "Overall:\t FDR-corrected: 58 \t Coincide with NIBS: 18\n"
     ]
    }
   ],
   "source": [
    "NIBS_file = pd.read_csv('../Data/NIBS_Identified_ROIs.csv')\n",
    "nibs = NIBS_file.loc[NIBS_file[dataset] == True, 'Node_ID'].tolist()\n",
    "print(dataset, len(nibs), nibs)\n",
    "\n",
    "print(Group1, len(GR1), '\\t', Group2, len(GR2), '\\tTotal:', len(GR1+GR2),'\\n')\n",
    "\n",
    "FDRcount, CoinNIBScount = 0,0\n",
    "for RSN in RSNs7:\n",
    "    rsn = RSN.replace(' ','')\n",
    "    indices = result_dict[RSN]\n",
    "    NNodes = len(indices)\n",
    "    df = pd.read_csv(outpath + f'p_values_{rsn}_NodalPSS_HomoScaffold.txt', sep = '\\t')\n",
    "    filtered_values = df.loc[df['fdr_corrected_p_val'] < 0.05, 'Nodes']\n",
    "    node_ids_fdr = [RSN_node_details[RSN][i] for i in filtered_values]\n",
    "    nodesno = set(nibs).intersection(set(node_ids_fdr))\n",
    "    FDRcount += len(filtered_values)\n",
    "    CoinNIBScount += len(nodesno)\n",
    "    print(\"FDR-corrected:\", len(filtered_values), '\\t Coincide with NIBS:', len(nodesno),'\\t', rsn)\n",
    "\n",
    "print('\\nOverall:\\t', \"FDR-corrected:\", FDRcount, '\\t Coincide with NIBS:', CoinNIBScount)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
